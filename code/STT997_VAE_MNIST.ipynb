{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c709b3-be84-49cc-9cea-91754f40e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# < define classes to build encoder and decoder networks >\n",
    "# We use encoder and decoder architectures in MNIST experiments in [1] (in C.1).\n",
    "# Other implementation details are adjusted to simplify the experiment.\n",
    "# [1] Tolstikhin, Ilya, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. \"Wasserstein auto-encoders.\"\n",
    "#    arXiv preprint arXiv:1711.01558 (2017).\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_x, dim_z, nf):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dim_z = dim_z\n",
    "        nc, resolution, _ = dim_x\n",
    "\n",
    "        # input size: nc x 28 x 28\n",
    "        self.c1 = nn.Sequential(\n",
    "                nn.Conv2d(nc, nf, 4, 2, 1),\n",
    "                nn.BatchNorm2d(nf),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "        # input size: nf x 14 x 14\n",
    "        self.c2 = nn.Sequential(\n",
    "                nn.Conv2d(nf, nf*2, 4, 2, 1),\n",
    "                nn.BatchNorm2d(nf*2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "        # input size: (nf*2) x 7 x 7\n",
    "        self.c3 = nn.Sequential(\n",
    "                nn.Conv2d(nf*2, nf*4, 4, 2, 2),\n",
    "                nn.BatchNorm2d(nf*4),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "        # input size: (nf*4) x 4 x 4\n",
    "        self.c4 = nn.Sequential(\n",
    "                nn.Conv2d(nf*4, nf*8, 4, 2, 1),\n",
    "                nn.BatchNorm2d(nf*8),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "        # input size: (nf*8) x 2 x 2\n",
    "        self.mu_net = nn.Conv2d(nf*8, dim_z, 2, 2, 0)\n",
    "        self.log_var_net = nn.Conv2d(nf*8, dim_z, 2, 2, 0)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        h1 = self.c1(x_input)\n",
    "        h2 = self.c2(h1)\n",
    "        h3 = self.c3(h2)\n",
    "        h4 = self.c4(h3)\n",
    "        mu, log_var = self.mu_net(h4), self.log_var_net(h4)\n",
    "        return mu.view(-1, self.dim_z), log_var.view(-1, self.dim_z)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_z, dim_x, nf, final_activation):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dim_z = dim_z\n",
    "        nc, h, _ = dim_x\n",
    "\n",
    "        # input size: dim_z x 1 x 1\n",
    "        self.upc1 = nn.Sequential(\n",
    "                  nn.ConvTranspose2d(dim_z, nf*8, 7, 1, 0),\n",
    "                  )\n",
    "        # input size: (nf*8) x 7 x 7\n",
    "        self.upc2 = nn.Sequential(\n",
    "                  nn.ConvTranspose2d(nf*8, nf*4, 4, 2, 1),\n",
    "                  nn.BatchNorm2d(nf*4),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  )\n",
    "        # input size: (nf*4) x 14 x 14\n",
    "        self.upc3 = nn.Sequential(\n",
    "                  nn.ConvTranspose2d(nf*4, nf*2, 4, 2, 1),\n",
    "                  nn.BatchNorm2d(nf*2),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  )\n",
    "        # input size: (nf*2) x 28 x 28\n",
    "        if final_activation == 'sigmoid':\n",
    "            self.mu_net = nn.Sequential(nn.ConvTranspose2d(nf*2, nc, 3, 1, 1),\n",
    "                                        nn.Sigmoid())\n",
    "        elif final_activation == 'tanh':\n",
    "            self.mu_net = nn.Sequential(nn.ConvTranspose2d(nf*2, nc, 3, 1, 1),\n",
    "                                        nn.Tanh())\n",
    "        elif final_activation == 'none':\n",
    "            self.mu_net = nn.Sequential(nn.ConvTranspose2d(nf*2, nc, 3, 1, 1))\n",
    "        else:\n",
    "            print('available choices for final_activation: sigmoid, tanh, and none')\n",
    "\n",
    "        self.obs_log_var_net = nn.ConvTranspose2d(1, nc, h, 1, 0)\n",
    "\n",
    "    def forward(self, z_input):\n",
    "        d1 = self.upc1(z_input.view(-1, self.dim_z, 1, 1))\n",
    "        d2 = self.upc2(d1)\n",
    "        d3 = self.upc3(d2)\n",
    "        o = self.mu_net(d3)\n",
    "\n",
    "        device = z_input.get_device()\n",
    "        if device == -1:\n",
    "            one_tensor = torch.ones((1,1,1,1))\n",
    "        else:\n",
    "            one_tensor = torch.ones((1,1,1,1)).cuda()\n",
    "\n",
    "        obs_log_var = self.obs_log_var_net(one_tensor)\n",
    "        return o, obs_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5525a-24ae-4032-b59f-638358b7cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# < define basic utility functions >\n",
    "def sampling(mean, log_var):\n",
    "    device = mean.get_device()\n",
    "    if device == -1:\n",
    "        epsilon = torch.randn(mean.shape)\n",
    "        return mean + torch.exp(0.5 * log_var) * epsilon\n",
    "    else:\n",
    "        epsilon = torch.randn(mean.shape).cuda()\n",
    "        return mean + torch.exp(0.5 * log_var).cuda() * epsilon\n",
    "\n",
    "def kl_criterion(mu1, log_var1, mu2, log_var2):\n",
    "    sigma1 = log_var1.mul(0.5).exp()\n",
    "    sigma2 = log_var2.mul(0.5).exp()\n",
    "    kld = torch.log(sigma2/sigma1) + ((sigma1**2) + (mu1 - mu2)**2)/(2.0*(sigma2**2)) - 0.5\n",
    "    return torch.sum(kld, dim=-1)\n",
    "\n",
    "def kl_annealing_weight(epoch, total_epochs):\n",
    "    return min(1, epoch / (0.1*total_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395310f-c41b-4c97-8c87-33bb1b55eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# < define advanced utility functions >\n",
    "def extract_feature(result_path, x, mean_extract=True):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    saved_model = torch.load('%s/model.pth' % result_path, weights_only=False)\n",
    "    encoder, decoder = saved_model['encoder'], saved_model['decoder']\n",
    "    encoder.eval(); decoder.eval()\n",
    "\n",
    "    if device == 'cuda':\n",
    "        z_mean, z_log_var = encoder(x.cuda())\n",
    "    elif device == 'cpu':\n",
    "        z_mean, z_log_var = encoder(x)\n",
    "    z_sample = sampling(z_mean, z_log_var)\n",
    "    if mean_extract:\n",
    "        return z_mean\n",
    "    else:\n",
    "        return z_sample\n",
    "\n",
    "def reconstruct(result_path, x, mean_extract=True):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    saved_model = torch.load('%s/model.pth' % result_path, weights_only=False)\n",
    "    encoder, decoder = saved_model['encoder'], saved_model['decoder']\n",
    "    encoder.eval(); decoder.eval()\n",
    "\n",
    "    if device == 'cuda':\n",
    "        z_mean, z_log_var = encoder(x.cuda())\n",
    "    elif device == 'cpu':\n",
    "        z_mean, z_log_var = encoder(x)\n",
    "    z_sample = sampling(z_mean, z_log_var)\n",
    "    if mean_extract:\n",
    "        x_recon = decoder(z_mean)[0]\n",
    "    else:\n",
    "        x_recon = decoder(z_sample)[0]\n",
    "    return x_recon\n",
    "\n",
    "def generate(result_path, z_sample):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    saved_model = torch.load('%s/model.pth' % result_path, weights_only=False)\n",
    "    decoder = saved_model['decoder']\n",
    "    decoder.eval()\n",
    "\n",
    "    if device == 'cuda':\n",
    "        x_gen = decoder(z_sample.cuda())[0]\n",
    "    elif device == 'cpu':\n",
    "        x_gen = decoder(z_sample)[0]\n",
    "    \n",
    "    return x_gen\n",
    "\n",
    "def plot_train_reconstruction(result_path, epoch, n_vis):\n",
    "    # extract features with trained VAE networks\n",
    "    z_train = extract_feature(result_path, torch.tensor(x_train[:n_vis], dtype=torch.float32))\n",
    "    z_train = z_train.detach().cpu().numpy()\n",
    "    \n",
    "    # compute reconstruction results\n",
    "    x_train_recon = reconstruct(result_path, torch.tensor(x_train[:n_vis], dtype=torch.float32))\n",
    "    x_train_recon = x_train_recon.cpu().detach().numpy()\n",
    "    \n",
    "    # plot and save reconstruction results\n",
    "    fig, axes = plt.subplots(2, n_vis, figsize=(12, 12))\n",
    "    for i in range(n_vis):\n",
    "        axes[0, i].imshow(x_train[i].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "        axes[1, i].imshow(x_train_recon[i].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        del(i)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s/train_reconstruction_results_%d.pdf' % (result_path, epoch), dpi=600)\n",
    "    plt.close()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def plot_test_reconstruction(result_path, epoch, n_vis):\n",
    "    # extract features with trained VAE networks\n",
    "    z_test = extract_feature(result_path, torch.tensor(x_test[:n_vis], dtype=torch.float32))\n",
    "    z_test = z_test.detach().cpu().numpy()\n",
    "    \n",
    "    # compute reconstruction results\n",
    "    x_test_recon = reconstruct(result_path, torch.tensor(x_test[:n_vis], dtype=torch.float32))\n",
    "    x_test_recon = x_test_recon.cpu().detach().numpy()\n",
    "    \n",
    "    # plot and save reconstruction results\n",
    "    fig, axes = plt.subplots(2, n_vis, figsize=(12, 12))\n",
    "    for i in range(n_vis):\n",
    "        axes[0, i].imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "        axes[1, i].imshow(x_test_recon[i].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        del(i)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s/test_reconstruction_results_%d.pdf' % (result_path, epoch), dpi=600)\n",
    "    plt.close()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def plot_generation(result_path, epoch, n_row, n_col):\n",
    "    n_row, n_col = 10, 10\n",
    "    x_gen = generate(result_path, torch.tensor(torch.randn(n_row*n_col, dim_z), dtype=torch.float32))\n",
    "    x_gen = x_gen.cpu().detach().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(12, 12))\n",
    "    for i in range(n_row):\n",
    "        for j in range(n_col):\n",
    "            axes[i, j].imshow(x_gen[i*n_row+j].reshape(28, 28), cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "        del(i, j)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s/generation_results_%d.pdf' % (result_path, epoch), dpi=600)\n",
    "    plt.close()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764029bd-4c41-41d3-a444-fde26d50055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import progressbar\n",
    "import tqdm\n",
    "\n",
    "def model(dim_x, dim_z, nf, decoder_final_activation='none'):\n",
    "    '''\n",
    "    dim_z: dimension of representations\n",
    "    nf: a factor to control the overall filter sizes in encoder and decoder networks\n",
    "    decoder_final_activation: the last activation layer in decoder.\n",
    "    '''\n",
    "\n",
    "    encoder = Encoder(dim_x, dim_z, nf)\n",
    "    decoder = Decoder(dim_z, dim_x, nf, final_activation=decoder_final_activation)\n",
    "    return [encoder, decoder]\n",
    "\n",
    "def fit(model, x_train, x_val, x_test,\n",
    "        num_epoch, batch_size, num_worker, seed,\n",
    "        beta_recon, beta_kl, Adam_beta1, Adam_beta2, weight_decay,\n",
    "        init_lr, lr_milestones, lr_gamma, val_period, recon_error,\n",
    "        dtype, result_path):\n",
    "    '''\n",
    "    num_epoch: the number of epoch\n",
    "    batch_size: the number of samples in each mini-batch\n",
    "    num_worker: the number of CPU cores\n",
    "    seed: the random seed number\n",
    "    beta_recon: the coefficient of the log-likelihood\n",
    "    beta_kl: the coefficient of the KL-penalty term in ELBOs\n",
    "    Adam_beta1: beta1 for Adam optimizer\n",
    "    Adam_beta2: beta2 for Adam optimizer\n",
    "    weight_decay: the coefficient of the half of L2 penalty term\n",
    "    init_lr: the initial learning rate\n",
    "    lr_milestones: the epochs to reduce the learning rate\n",
    "    lr_gamma: the multiplier for each time learning rate is reduced\n",
    "    val_period: the frequency of evaluating trained models\n",
    "    recon_error: the type of reconstruction error (supports 'mse' and 'likelihood')\n",
    "    dtype: the data type\n",
    "    result_path: the directory where results are saved. the directory where results are saved. Its default value is results/vae-time=month-day-hour-min-sec.\n",
    "    '''\n",
    "\n",
    "    # declare basic variables\n",
    "    encoder, decoder = model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    Adam_betas = (Adam_beta1, Adam_beta2)\n",
    "    if result_path is None:\n",
    "        now = datetime.datetime.now()\n",
    "        result_path = './results/vae-time=%d-%d-%d-%d-%d' % (now.month, now.day, now.hour, now.minute, now.second)\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    # lines for reproducibility\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # convert data to tensors\n",
    "    x_train = torch.tensor(x_train, dtype=dtype)\n",
    "    x_val = torch.tensor(x_val, dtype=dtype)\n",
    "    x_test = torch.tensor(x_test, dtype=dtype)\n",
    "\n",
    "    # define optimizers and schedulers\n",
    "    enc_optimizer = torch.optim.Adam(encoder.parameters(), betas=Adam_betas, lr=init_lr, weight_decay=weight_decay)\n",
    "    gen_optimizer = torch.optim.Adam(decoder.parameters(), betas=Adam_betas, lr=init_lr, weight_decay=weight_decay)\n",
    "\n",
    "    enc_scheduler = torch.optim.lr_scheduler.MultiStepLR(enc_optimizer, milestones=lr_milestones, gamma=lr_gamma)\n",
    "    gen_scheduler = torch.optim.lr_scheduler.MultiStepLR(gen_optimizer, milestones=lr_milestones, gamma=lr_gamma)\n",
    "\n",
    "    # define training log\n",
    "    loss_names = ['loss', 'recon_loss', 'kl_post_prior', 'l2_penalty']\n",
    "    logs = {}\n",
    "    for datasetname in ['train', 'val', 'test']:\n",
    "        logs[datasetname] = {}\n",
    "        for loss_name in loss_names:\n",
    "            logs[datasetname][loss_name] = []\n",
    "        del(loss_name)\n",
    "    del(datasetname)\n",
    "    summary_stats = []\n",
    "\n",
    "    # define data loader\n",
    "    dataloader = {}\n",
    "    dataloader['train'] = DataLoader(x_train, batch_size=batch_size, num_workers=num_worker, shuffle=True)\n",
    "    dataloader['val'] = DataLoader(x_val, batch_size=batch_size, num_workers=num_worker)\n",
    "    dataloader['test'] = DataLoader(x_test, batch_size=batch_size, num_workers=num_worker)\n",
    "\n",
    "    # training part\n",
    "    mse_criterion = torch.nn.MSELoss()\n",
    "    if device == 'cuda':\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "        mse_criterion.cuda()\n",
    "\n",
    "    best_val_epoch=1; best_val_loss = 10**5\n",
    "    for epoch in range(1, num_epoch+1):\n",
    "        num_batch = 0\n",
    "        for x_batch in tqdm.tqdm(dataloader['train'], desc='[Epoch %d/%d] Training' % (epoch, num_epoch)):\n",
    "            num_batch += 1\n",
    "            if device == 'cuda':\n",
    "                x_batch = x_batch.cuda()\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "\n",
    "            enc_optimizer.zero_grad()\n",
    "            gen_optimizer.zero_grad()\n",
    "\n",
    "            # forward step\n",
    "            z_mean, z_log_var = encoder(x_batch)\n",
    "            z_sample = sampling(z_mean, z_log_var)\n",
    "            if device == 'cuda':\n",
    "                z_sample = z_sample.cuda()\n",
    "            fire_rate, obs_log_var = decoder(z_sample)\n",
    "            \n",
    "            # compute objective function\n",
    "            kl_weight = kl_annealing_weight(epoch, num_epoch)\n",
    "            if recon_error=='mse':\n",
    "                obs_loglik = -0.5*torch.sum((fire_rate - x_batch)**2, dim=(1, 2, 3))\n",
    "            elif recon_error=='likelihood':\n",
    "                obs_loglik = torch.sum(-0.5 * (obs_log_var + (x_batch - fire_rate)**2 / torch.exp(obs_log_var)), dim=(1, 2, 3))\n",
    "            kl_post_prior = kl_criterion(z_mean, z_log_var, torch.zeros_like(z_mean), torch.zeros_like(z_log_var))\n",
    "\n",
    "            elbo = beta_recon*obs_loglik - beta_kl*kl_weight*kl_post_prior\n",
    "            loss = torch.mean(-elbo)\n",
    "\n",
    "            # backward step\n",
    "            loss.backward()\n",
    "\n",
    "            enc_optimizer.step()\n",
    "            gen_optimizer.step()\n",
    "        del(x_batch)\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        for datasetname in ['train', 'val', 'test']:\n",
    "            loss_cumsum, sample_size = 0.0, 0\n",
    "            obs_loglik_cumsum, kl_post_prior_cumsum = 0.0, 0.0\n",
    "            for x_batch in tqdm.tqdm(dataloader[datasetname],\n",
    "                                     desc='[Epoch %d/%d] Computing loss terms on %s' % (epoch, num_epoch, datasetname)):\n",
    "                x_batch = x_batch.cuda() if device == 'cuda' else x_batch\n",
    "\n",
    "                # forward step\n",
    "                z_mean, z_log_var = encoder(x_batch)\n",
    "                z_sample = sampling(z_mean, z_log_var)\n",
    "                if device == 'cuda':\n",
    "                    z_sample = z_sample.cuda()\n",
    "                fire_rate, obs_log_var = decoder(z_sample)\n",
    "\n",
    "                # compute objective function\n",
    "                if recon_error=='mse':\n",
    "                    obs_loglik = -0.5*torch.sum((fire_rate - x_batch)**2, dim=(1, 2, 3))\n",
    "                elif recon_error=='likelihood':\n",
    "                    obs_loglik = torch.sum(-0.5 * (obs_log_var + (x_batch - fire_rate)**2 / torch.exp(obs_log_var)), dim=(1, 2, 3))\n",
    "                kl_post_prior = kl_criterion(z_mean, z_log_var, torch.zeros_like(z_mean), torch.zeros_like(z_log_var))\n",
    "\n",
    "                elbo = beta_recon*obs_loglik - beta_kl*kl_post_prior\n",
    "                loss = torch.mean(-elbo)\n",
    "\n",
    "                loss_cumsum += loss.item()*np.shape(x_batch)[0]\n",
    "                obs_loglik_cumsum += torch.mean(obs_loglik).item()*np.shape(x_batch)[0]\n",
    "                kl_post_prior_cumsum += torch.mean(kl_post_prior).item()*np.shape(x_batch)[0]\n",
    "                sample_size += np.shape(x_batch)[0]\n",
    "            del(x_batch)\n",
    "\n",
    "            l2_penalty = 0.0\n",
    "            for networks in [encoder, decoder]:\n",
    "                for name, m in networks.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        l2_penalty += 0.5*torch.sum(m**2)\n",
    "\n",
    "            logs[datasetname]['loss'].append(loss_cumsum/sample_size)\n",
    "            logs[datasetname]['recon_loss'].append(-obs_loglik_cumsum/sample_size)\n",
    "            logs[datasetname]['kl_post_prior'].append(kl_post_prior_cumsum/sample_size)\n",
    "            logs[datasetname]['l2_penalty'].append(l2_penalty.item())\n",
    "\n",
    "        if epoch % val_period == 0:\n",
    "            # save loss curves\n",
    "            plt.figure()\n",
    "            linestyles = ['solid', 'dashed', 'dotted']\n",
    "            i = 0\n",
    "            for dataset_name in ['train', 'val', 'test']:\n",
    "                plt.plot(logs[dataset_name]['loss'][:], linestyle=linestyles[i],\n",
    "                         label=dataset_name)\n",
    "                i += 1\n",
    "            del(i)\n",
    "            plt.legend()\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.savefig('%s/loss_curves.pdf' % (result_path), dpi=600)\n",
    "            plt.close()\n",
    "\n",
    "            # update models and logs if the best validation loss is updated\n",
    "            current_val_loss = logs['val']['loss'][-1]\n",
    "            best_val_loss = np.minimum(best_val_loss, current_val_loss)\n",
    "            if best_val_loss == current_val_loss:\n",
    "                # update model and logs\n",
    "                best_val_epoch = epoch\n",
    "                os.makedirs('%s/' % result_path, exist_ok=True)\n",
    "                torch.save({'encoder': encoder, 'decoder': decoder, 'logs': logs,\n",
    "                            'num_epoch': num_epoch, 'batch_size': batch_size,\n",
    "                            'num_worker': num_worker, 'seed': seed,\n",
    "                            'beta_recon': beta_recon, 'beta_kl': beta_kl,\n",
    "                            'Adam_beta1': Adam_beta1, 'Adam_beta2': Adam_beta2,\n",
    "                            'weight_decay': weight_decay, 'init_lr': init_lr,\n",
    "                            'lr_milestones': lr_milestones, 'lr_gamma': lr_gamma,\n",
    "                            'val_period': val_period, 'recon_error': recon_error,\n",
    "                            'dtype': dtype, 'result_path': result_path},\n",
    "                            '%s/model.pth' % result_path)\n",
    "                \n",
    "                # draw and save reconstruction and generation results\n",
    "                plot_train_reconstruction(result_path, epoch, n_vis=10)\n",
    "                plot_test_reconstruction(result_path, epoch, n_vis=10)\n",
    "                plot_generation(result_path, epoch, n_row=10, n_col=10)\n",
    "                \n",
    "            if epoch == num_epoch:\n",
    "                # update logs\n",
    "                saved_model = torch.load('%s/model.pth' % result_path, weights_only=False)\n",
    "                saved_model['logs'] = logs\n",
    "                torch.save(saved_model, '%s/model.pth' % result_path)\n",
    "                del(saved_model)\n",
    "\n",
    "            current_summary_stats_row = {}\n",
    "\n",
    "            current_summary_stats_row['epoch'] = epoch\n",
    "            current_summary_stats_row['best_val_epoch'] = best_val_epoch\n",
    "            current_summary_stats_row['train_loss'] = logs['train']['loss'][-1]\n",
    "            current_summary_stats_row['val_loss'] = logs['val']['loss'][-1]\n",
    "            current_summary_stats_row['test_loss'] = logs['test']['loss'][-1]\n",
    "            current_summary_stats_row['train_recon_loss'] = logs['train']['recon_loss'][-1]\n",
    "            current_summary_stats_row['val_recon_loss'] = logs['val']['recon_loss'][-1]\n",
    "            current_summary_stats_row['test_recon_loss'] = logs['test']['recon_loss'][-1]\n",
    "            current_summary_stats_row['train_kl_post_prior'] = logs['train']['kl_post_prior'][-1]\n",
    "            current_summary_stats_row['val_kl_post_prior'] = logs['val']['kl_post_prior'][-1]\n",
    "            current_summary_stats_row['test_kl_post_prior'] = logs['test']['kl_post_prior'][-1]\n",
    "            current_summary_stats_row['l2_penalty'] = logs['train']['l2_penalty'][-1]\n",
    "\n",
    "            summary_stats.append(current_summary_stats_row)\n",
    "            pd.DataFrame(summary_stats).to_csv('%s/summary_stats.csv' % result_path, index=False)\n",
    "    del(epoch)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71027cc-96e4-451e-9a1c-28629e2652c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# variables for data\n",
    "p_train = 0.80\n",
    "p_val = 1.0 - p_train\n",
    "\n",
    "# variables for VAE architectures\n",
    "dim_z = 8 # dimension of representations\n",
    "nf = 64 # a factor to control the overall filter sizes in encoder and decoder networks\n",
    "decoder_final_activation='none' # the last activation layer in decoder.\n",
    "\n",
    "# variables for optimization\n",
    "seed = 0 # the random seed number\n",
    "num_epoch = 30 # the number of epoch\n",
    "batch_size = 64 # the number of samples in each mini-batch\n",
    "num_worker = 8 # the number of CPU cores\n",
    "beta_recon = 1.0 # the coefficient of the log-likelihood\n",
    "beta_kl = 1.0 # the coefficient of the KL-penalty term in ELBOs\n",
    "Adam_beta1 = 0.5 # beta1 for Adam optimizer\n",
    "Adam_beta2 = 0.999 # beta2 for Adam optimizer\n",
    "weight_decay = 5e-6 # the coefficient of the half of L2 penalty term\n",
    "init_lr = 2e-4 # the initial learning rate\n",
    "lr_milestones = [10, 20] # the epochs to reduce the learning rate\n",
    "lr_gamma = 0.5 # the multiplier for each time learning rate is reduced\n",
    "val_period = 1 # the frequency of evaluating trained models\n",
    "recon_error = 'mse' # the type of reconstruction error (supports 'mse' and 'likelihood')\n",
    "dtype = torch.float32 # the data type\n",
    "result_path = None # the directory where results are saved. Its default value is results/vae-time=month-day-hour-min-sec.\n",
    "\n",
    "# < reproducibility >\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# < load and split MNIST data >\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform, download=True)\n",
    "\n",
    "# split the official MNIST training dataset into training and validation\n",
    "num_train = int(p_train * len(train_dataset))\n",
    "num_val = len(train_dataset) - num_train\n",
    "train_dataset, val_dataset = random_split(train_dataset, [num_train, num_val])\n",
    "\n",
    "# Function to extract and convert data from a dataset\n",
    "def extract_data(dataset):\n",
    "    # Because dataset is a Subset, we access .dataset to get original dataset properties\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    return images.numpy(), labels.numpy()\n",
    "\n",
    "# Extracting data\n",
    "x_train, y_train = extract_data(train_dataset)\n",
    "x_val, y_val = extract_data(val_dataset)\n",
    "x_test, y_test = extract_data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f55e06-65e8-493a-8804-683e12a8382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build VAE networks\n",
    "dim_x = np.shape(x_train)[1:]\n",
    "vae = model(dim_x=dim_x, dim_z=dim_z, nf=nf, decoder_final_activation=decoder_final_activation)\n",
    "\n",
    "# train VAE networks. Results will be saved at the result_path\n",
    "start_time = time.time()\n",
    "fit(model=vae, x_train=x_train, x_val=x_val, x_test=x_test,\n",
    "    num_epoch=num_epoch, batch_size=batch_size, num_worker=num_worker, seed=seed,\n",
    "    beta_recon=beta_recon, beta_kl=beta_kl,\n",
    "    Adam_beta1=Adam_beta1, Adam_beta2=Adam_beta2, weight_decay=weight_decay,\n",
    "    init_lr=init_lr, lr_milestones=lr_milestones, lr_gamma=lr_gamma,\n",
    "    val_period=val_period, recon_error=recon_error,\n",
    "    dtype=dtype, result_path=result_path)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Total training time: {training_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
